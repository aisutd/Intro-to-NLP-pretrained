{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word embedding models - GloVe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9AZs5705mTv"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iT4c5wI7JEE"
      },
      "source": [
        "#####################################################################\r\n",
        "###################  use pretrained model: GloVe  ###################\r\n",
        "#####################################################################\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "######################### install environment ##########################\r\n",
        "## install environment is needed\r\n",
        "## pip install numpy\r\n",
        "## pip install scipy\r\n",
        "## pip install matplotlib\r\n",
        "## pip install sklearn\r\n",
        "\r\n",
        "\r\n",
        "############################  Resources  #################################\r\n",
        "## file download link: (Wikipedia 2014 + Gigaword 5 vectors) \r\n",
        "## http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "\r\n",
        "## other GloVe download page:\r\n",
        "## https://nlp.stanford.edu/projects/glove/\r\n",
        "\r\n",
        "## ref: https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db\r\n",
        "\r\n",
        "######################## if use google colab #########################\r\n",
        "## if use google colab: use the following code to download and unzip it.\r\n",
        "## if do not use google colab, jump to the next block\r\n",
        "import os\r\n",
        "import urllib.request\r\n",
        "import zipfile\r\n",
        "source_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\r\n",
        "zip_name = \"GloVe.zip\"\r\n",
        "if (os.path.exists(zip_name) == False):\r\n",
        "    urllib.request.urlretrieve(source_url,zip_name)\r\n",
        "data_dir = \"GloVe\"\r\n",
        "with zipfile.ZipFile(zip_name, 'r') as zip_ref:\r\n",
        "    zip_ref.extractall(data_dir)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "############################ library used ###########################\r\n",
        "## if do not use google colab, download data from the resource block \r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "########################## create a dictionary #######################\r\n",
        "## to create a dictionary that can: embeddings_dict[input_your_word] --> output its embedding\r\n",
        "## create a empty dictionary\r\n",
        "embeddings_dict = {}\r\n",
        "## open file as f\r\n",
        "filePath = \"GloVe/glove.6B.50d.txt\"\r\n",
        "with open(filePath, 'r') as f:\r\n",
        "    ## for each line, transfer a string like: 'business 0.023693 0.13316 0.023131' \r\n",
        "    ##                  into a list like ['business', '0.023693', '0.13316', '0.023131']\r\n",
        "    for line in f:\r\n",
        "        values = line.split()\r\n",
        "        word = values[0]\r\n",
        "        ## then, put all the number into an np.array using np.asarray(input_array, data_type)\r\n",
        "        vector = np.asarray(values[1:], dtype=\"float32\")\r\n",
        "        embeddings_dict[word] = vector\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I93T2SB_ETz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457a5f92-23a9-4c99-f48d-b94e467f9d0a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.52042 , -0.8314  ,  0.49961 ,  1.2893  ,  0.1151  ,  0.057521,\n",
              "       -1.3753  , -0.97313 ,  0.18346 ,  0.47672 , -0.15112 ,  0.35532 ,\n",
              "        0.25912 , -0.77857 ,  0.52181 ,  0.47695 , -1.4251  ,  0.858   ,\n",
              "        0.59821 , -1.0903  ,  0.33574 , -0.60891 ,  0.41742 ,  0.21569 ,\n",
              "       -0.07417 , -0.5822  , -0.4502  ,  0.17253 ,  0.16448 , -0.38413 ,\n",
              "        2.3283  , -0.66682 , -0.58181 ,  0.74389 ,  0.095015, -0.47865 ,\n",
              "       -0.84591 ,  0.38704 ,  0.23693 , -1.5523  ,  0.64802 , -0.16521 ,\n",
              "       -1.4719  , -0.16224 ,  0.79857 ,  0.97391 ,  0.40027 , -0.21912 ,\n",
              "       -0.30938 ,  0.26581 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}
